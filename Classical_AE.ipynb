{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4078590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for data processing and machine learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,\n",
    "                             recall_score, f1_score, roc_auc_score)\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# ConvergenceWarning 무시\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea7c1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 946 samples, 30 features\n",
      "Fraud rate: 0.5000 (473 fraud cases)\n",
      "\n",
      "Training set: (756, 4)\n",
      "Test set: (190, 4)\n",
      "PCA explained variance ratio: [0.38421646 0.10954544 0.06067923 0.05752846]\n",
      "Total variance explained: 0.6120\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Data Loading and Preprocessing Pipeline\n",
    "# ==========================================\n",
    "\n",
    "# Load preprocessed credit card fraud dataset\n",
    "df = pd.read_csv(\"preprocessed-creditcard.csv\")\n",
    "X = df.drop(\"Class\", axis=1).values  # Feature matrix\n",
    "y = df[\"Class\"].values                # Target labels (0: normal, 1: fraud)\n",
    "\n",
    "print(f\"Dataset loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Fraud rate: {np.mean(y):.4f} ({np.sum(y)} fraud cases)\")\n",
    "\n",
    "# Stratified train-test split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Feature standardization using Z-score normalization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# Dimensionality reduction using PCA to match quantum register size\n",
    "pca = PCA(n_components=4, random_state=42)\n",
    "X_train_4d = pca.fit_transform(X_train)\n",
    "X_test_4d  = pca.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_4d.shape}\")\n",
    "print(f\"Test set: {X_test_4d.shape}\")\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {np.sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac6b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Configuration: {'epochs_classical': 100, 'batch_size_classical': 16, 'learning_rate': 0.001}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Configuration\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# TRAINING CONFIGURATION\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs_classical': 100,\n",
    "    'batch_size_classical' : 16,\n",
    "    'learning_rate': 0.001      # Adam optimizer stepsize\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining Configuration: {TRAINING_CONFIG}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b6f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classical_batch_cost(samples, model):\n",
    "    \"\"\"\n",
    "    Classical AE용 배치 손실 계산 함수.\n",
    "    \n",
    "    Args:\n",
    "        samples: (배치) 원본 데이터, shape = (batch_size, n_features)\n",
    "        model: 학습된 MLPRegressor 오토인코더\n",
    "    \n",
    "    Returns:\n",
    "        linear_loss: 평균 절댓값 오차 (MAE)\n",
    "        squared_loss: 평균 제곱 오차 (MSE)\n",
    "    \"\"\"\n",
    "    recon = model.predict(samples)\n",
    "    errors = recon - samples\n",
    "    linear_loss   = np.mean(np.abs(errors))\n",
    "    squared_loss  = np.mean(errors**2)\n",
    "    return linear_loss, squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d3ce320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classical_ae_strategy():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING: CLASSICAL AUTOENCODER STRATEGY\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    ae = MLPRegressor(\n",
    "        hidden_layer_sizes=(2),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        learning_rate_init=TRAINING_CONFIG['learning_rate'],\n",
    "        max_iter=1,           # 한 에포크씩 학습\n",
    "        warm_start=True,\n",
    "        batch_size=TRAINING_CONFIG['batch_size_classical'],\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    linear_losses  = []\n",
    "    squared_losses = []\n",
    "\n",
    "    for epoch in range(1, TRAINING_CONFIG['epochs_classical']+1):\n",
    "        # 한 에포크 학습\n",
    "        ae.fit(X_train_4d, X_train_4d)\n",
    "\n",
    "        # 전체 학습 세트에 대한 loss 계산\n",
    "        lin_loss, sq_loss = compute_classical_batch_cost(X_train_4d, ae)\n",
    "        linear_losses.append(lin_loss)\n",
    "        squared_losses.append(sq_loss)\n",
    "\n",
    "        # 5 에포크마다 출력\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print(f\"  Epoch {epoch:2d}/{TRAINING_CONFIG['epochs_classical']} — \"\n",
    "                  f\"Linear Loss: {lin_loss:.6f}, \"\n",
    "                  f\"Squared Loss: {sq_loss:.6f}\")\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    recon_test  = ae.predict(X_test_4d)\n",
    "    test_errors = np.mean((X_test_4d - recon_test)**2, axis=1)\n",
    "    threshold   = np.percentile(\n",
    "        np.mean((X_train_4d - ae.predict(X_train_4d))**2, axis=1),\n",
    "        TRAINING_CONFIG.get('threshold_percentile', 95)\n",
    "    )\n",
    "    y_pred = (test_errors > threshold).astype(int)\n",
    "\n",
    "    print(f\"\\n  - Threshold (95th percentile of train MSE): {threshold:.6f}\")\n",
    "    print(f\"  - Test set anomaly rate: {y_pred.mean():.4f}\")\n",
    "\n",
    "    return {\n",
    "        'strategy'       : 'classical_ae',\n",
    "        'model'          : ae,\n",
    "        'threshold'      : threshold,\n",
    "        'y_pred'         : y_pred,\n",
    "        'linear_losses'  : linear_losses,\n",
    "        'squared_losses' : squared_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50deb8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING: CLASSICAL AUTOENCODER STRATEGY\n",
      "============================================================\n",
      "  Epoch  1/100 — Linear Loss: 1.710141, Squared Loss: 6.008520\n",
      "  Epoch  5/100 — Linear Loss: 1.370326, Squared Loss: 4.306587\n",
      "  Epoch 10/100 — Linear Loss: 1.100025, Squared Loss: 2.972238\n",
      "  Epoch 15/100 — Linear Loss: 0.921455, Squared Loss: 2.203886\n",
      "  Epoch 20/100 — Linear Loss: 0.811215, Squared Loss: 1.832543\n",
      "  Epoch 25/100 — Linear Loss: 0.736866, Squared Loss: 1.634872\n",
      "  Epoch 30/100 — Linear Loss: 0.687462, Squared Loss: 1.514719\n",
      "  Epoch 35/100 — Linear Loss: 0.643693, Squared Loss: 1.432691\n",
      "  Epoch 40/100 — Linear Loss: 0.602901, Squared Loss: 1.374535\n",
      "  Epoch 45/100 — Linear Loss: 0.566910, Squared Loss: 1.332252\n",
      "  Epoch 50/100 — Linear Loss: 0.537200, Squared Loss: 1.302266\n",
      "  Epoch 55/100 — Linear Loss: 0.520728, Squared Loss: 1.283323\n",
      "  Epoch 60/100 — Linear Loss: 0.512190, Squared Loss: 1.270455\n",
      "  Epoch 65/100 — Linear Loss: 0.507241, Squared Loss: 1.260618\n",
      "  Epoch 70/100 — Linear Loss: 0.504338, Squared Loss: 1.252431\n",
      "  Epoch 75/100 — Linear Loss: 0.502298, Squared Loss: 1.245575\n",
      "  Epoch 80/100 — Linear Loss: 0.501040, Squared Loss: 1.239669\n",
      "  Epoch 85/100 — Linear Loss: 0.500152, Squared Loss: 1.234285\n",
      "  Epoch 90/100 — Linear Loss: 0.499647, Squared Loss: 1.229230\n",
      "  Epoch 95/100 — Linear Loss: 0.499400, Squared Loss: 1.224349\n",
      "  Epoch 100/100 — Linear Loss: 0.499404, Squared Loss: 1.219560\n",
      "\n",
      "  - Threshold (95th percentile of train MSE): 2.511278\n",
      "  - Test set anomaly rate: 0.0579\n",
      "✓ classical strategy completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Train Angle strategy\n",
    "results = {}\n",
    "total_start_time = time.time()\n",
    "try:\n",
    "    classical_result = train_classical_ae_strategy()\n",
    "    results['classical'] = classical_result\n",
    "    print(f\"✓ classical strategy completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ classical strategy failed: {str(e)}\")\n",
    "    results['classical'] = {'error': str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38b06007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity statistics:\n",
      "  Mean: -0.1032\n",
      "  Std:  4.7260\n",
      "  Min:  -52.9669\n",
      "  Max:  0.9990\n",
      "\n",
      "Threshold optimization:\n",
      "  T=0.3: Acc=0.616 Prec=0.806 Rec=0.305 F1=0.443 G-Mean=0.532\n",
      "  T=0.4: Acc=0.637 Prec=0.795 Rec=0.368 F1=0.504 G-Mean=0.578\n",
      "  T=0.5: Acc=0.658 Prec=0.800 Rec=0.421 F1=0.552 G-Mean=0.614\n",
      "  T=0.6: Acc=0.700 Prec=0.817 Rec=0.516 F1=0.632 G-Mean=0.675\n",
      "  T=0.7: Acc=0.742 Prec=0.795 Rec=0.653 F1=0.717 G-Mean=0.737\n",
      "  T=0.8: Acc=0.753 Prec=0.767 Rec=0.726 F1=0.746 G-Mean=0.752\n",
      "\n",
      "RESULTS SUMMARY:\n",
      "  AUC-ROC Score: 0.7935\n",
      "  Best Threshold: 0.8 (G-Mean: 0.752)\n",
      "  Best Performance: Acc=0.753, Prec=0.767, Rec=0.726, F1=0.746\n"
     ]
    }
   ],
   "source": [
    "# Compute test reconstruction MSE and convert to “fidelity”\n",
    "recon_test = classical_result['model'].predict(X_test_4d)\n",
    "errors = np.mean((X_test_4d - recon_test)**2, axis=1)\n",
    "fidelities = 1.0 - errors\n",
    "\n",
    "print(\"Fidelity statistics:\")\n",
    "print(f\"  Mean: {fidelities.mean():.4f}\")\n",
    "print(f\"  Std:  {fidelities.std():.4f}\")\n",
    "print(f\"  Min:  {fidelities.min():.4f}\")\n",
    "print(f\"  Max:  {fidelities.max():.4f}\\n\")\n",
    "\n",
    "# Threshold optimization\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "print(\"Threshold optimization:\")\n",
    "for T in thresholds:\n",
    "    y_pred = (fidelities < T).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "    spec = tn / (tn + fp) if (tn + fp) else 0.0\n",
    "    gmean = (rec * spec)**0.5\n",
    "    print(f\"  T={T:.1f}: Acc={acc:.3f} Prec={prec:.3f} Rec={rec:.3f}\"\n",
    "          f\" F1={f1:.3f} G-Mean={gmean:.3f}\")\n",
    "\n",
    "# Results summary\n",
    "auc = roc_auc_score(y_test, 1.0 - fidelities)\n",
    "best_T, best_g = max(\n",
    "    ((T, (recall_score(y_test, (fidelities<T).astype(int), zero_division=0) *\n",
    "         (confusion_matrix(y_test, (fidelities<T).astype(int), labels=[0,1]).ravel()[0] /\n",
    "          (confusion_matrix(y_test, (fidelities<T).astype(int), labels=[0,1]).ravel()[0] +\n",
    "           confusion_matrix(y_test, (fidelities<T).astype(int), labels=[0,1]).ravel()[1])))**0.5)\n",
    "     for T in thresholds),\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "# Compute metrics at best threshold\n",
    "y_best = (fidelities < best_T).astype(int)\n",
    "m = {\n",
    "    'Accuracy': accuracy_score(y_test, y_best),\n",
    "    'Precision': precision_score(y_test, y_best, zero_division=0),\n",
    "    'Recall': recall_score(y_test, y_best, zero_division=0),\n",
    "    'F1': f1_score(y_test, y_best, zero_division=0),\n",
    "    'Gmean': (recall_score(y_test, y_best, zero_division=0) *\n",
    "              (confusion_matrix(y_test, y_best, labels=[0,1]).ravel()[0] /\n",
    "               (confusion_matrix(y_test, y_best, labels=[0,1]).ravel()[0] +\n",
    "                confusion_matrix(y_test, y_best, labels=[0,1]).ravel()[1])))**0.5\n",
    "}\n",
    "\n",
    "print(\"\\nRESULTS SUMMARY:\")\n",
    "print(f\"  AUC-ROC Score: {auc:.4f}\")\n",
    "print(f\"  Best Threshold: {best_T:.1f} (G-Mean: {best_g:.3f})\")\n",
    "print(f\"  Best Performance: Acc={m['Accuracy']:.3f}, Prec={m['Precision']:.3f},\"\n",
    "      f\" Rec={m['Recall']:.3f}, F1={m['F1']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
